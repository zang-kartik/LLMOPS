
# **Tools for LLMOps**

1. **Stanford Alpaca 7B** -> A model fine-tuned from the LLaMA 7B model, on 52k instruction following demonstration. Alpaca behaves qualitatively similar to OpenAI' text-davinci-003, while being surprisingly easy and cheap to reproduce.

2. **BLOOM** ->  Bloom is a autoregressive LLM trained to continue text from a prompt on vast amounts of text data using industrial scale computational scale. Bloom can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation task. It is a transformer based model, developed by bigscience, funded by hugging face. 

3. **Dolly** -> Dolly is a instruction following large language model,trained on databricks ML platform,that is licensed for commercial use. Dolly is trained on ~15k instruction/response fine tuning records generated by databricks employees in capability domains from InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization.

4. **FastChat** -> An open platform from training, serving and evaluating Large language model based chatbots. The core features include weights, training code, and evaluation code for state of art models eg vicuna. A distributed multi model serving system with web UI and OpenAI compatible restful API. Vicuna is based on LlaMA 2 with 4k and 16k context length.

5. **ChatGLM-6B** -> 

